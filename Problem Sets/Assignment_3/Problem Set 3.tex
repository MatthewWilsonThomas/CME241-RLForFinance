\documentclass[11pt]{article}

% Load package
\usepackage{../lesson}
\setcounter{MaxMatrixCols}{20}

% Set title and course name
\settitle{Problem Set 3}
\setsubtitle{MDP and Dynamic Programmings}
\setcourse{CME241 - Reinforcement Learning for Finance}
\setdate{Due: 23.01.2023}

\begin{document}

% Create title and add proper header for first page
\maketitle
\thispagestyle{first}

\section{Analytic Optimal Actions and Cost}
\begin{align}
    s\in{\mathcal{S}}&,\quad \mathcal{S} = \mathbb{R}\\
    a\in{\mathcal{A}}&,\quad \mathcal{A} = \mathbb{R}
\end{align}

We also have that $s' \sim \mathcal{N}(s, \sigma^2)$. 

Our goal is to minimize the Infinite-Horizeon Expected Discounted Sum of Costs. To do this we define:
$$
C = \sum_{i=0}^\infty \gamma^ie^{as'_i}
$$
We can they say that since $\gamma = 0$ all future costs are ignored giving:
$$
C = e^{as'_0}
$$
The expectation of this is thus:
$$
\E{C} = \E{e^{as'_0}}
$$

We also know that $s'\sim\mathcal{N}(s, \sigma^2)$ giving $as'\sim\mathcal{N}(as, a^2\sigma^2)$. This gives us a distribution for the exponential as the log-Normal distribution $\sim logNormal(as', a^2\sigma^2)$

This gives us a closed form expectations:
$$
\E{C} = e^{as + \frac{a^2\sigma^2}{2}}
$$
Minimizing this cost:
\begin{align}
    \min_{a\in\mathcal{A}} \E{C} &= \min_{a\in\mathcal{A}} e^{as + \frac{a^2\sigma^2}{2}}\\
    &= \min_{a\in\mathcal{A}} as + \frac{a^2\sigma^2}{2} \\
    \frac{\partial}{\partial a}\left(as + \frac{a^2\sigma^2}{2}\right) &= s + a\sigma^2 \\
    \text{giving optimality as}\quad a &= -\frac{s}{\sigma^2}\\
    \text{the optimal policy therefore leads to }\quad \E{C} &= e^{as + \frac{a^2\sigma^2}{2}} \\
    &= e^{-\frac{s^2}{\sigma^2} + \frac{s^2\sigma^2}{\sigma^4}} \\
    &= e^{-\frac{s^2}{2\sigma^2}}
\end{align}

%----------------------------------------------
\newpage
\section{Manual Value Iteration}
\begin{align}
    \mathcal{S} &= \{s_1, s_2, s_3\} \\
    \mathcal{T} &= \{s_3\} \\
    \mathcal{A} &= \{a_1, a_2\}
\end{align}

We have the Value Iteration Update as:
$$
V_{i+1}(s)=B^*\left(V_i\right)(s) \text { for all } s \in \mathcal{N}
$$
with
$$
\boldsymbol{B}^*(\boldsymbol{V})(s)=\max _{a \in \mathcal{A}}\left\{\mathcal{R}(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot \boldsymbol{V}\left(s^{\prime}\right)\right\}
$$

We also know the starting Value Function:
\begin{align}
    V_0(s) = \begin{bmatrix}
        10 \\
        1\\
        0
    \end{bmatrix}
\end{align}

Using the value iteration update we can then write:
\begin{align}
    V_1(s) = \begin{bmatrix}
        \max_{a\in\mathcal{A}} \left\{\mathcal{R}(s_1, a) + \sum_{s' \in \mathcal{N}} \mathcal{P}(s_1, a, s') \cdot \boldsymbol{V}(s') \right\}\\
        \max_{a\in\mathcal{A}} \left\{\mathcal{R}(s_2, a) + \sum_{s' \in \mathcal{N}} \mathcal{P}(s_2, a, s') \cdot \boldsymbol{V}(s') \right\}\\
        0
    \end{bmatrix} = \begin{bmatrix}
        11.2\\
        4.3\\
        0
    \end{bmatrix}
\end{align}
Then, by iterating again:
\begin{align}
    V_2(s) = \begin{bmatrix}
        \max_{a\in\mathcal{A}} \left\{\mathcal{R}(s_1, a) + \sum_{s' \in \mathcal{N}} \mathcal{P}(s_1, a, s') \cdot \boldsymbol{V}(s') \right\}\\
        \max_{a\in\mathcal{A}} \left\{\mathcal{R}(s_2, a) + \sum_{s' \in \mathcal{N}} \mathcal{P}(s_2, a, s') \cdot \boldsymbol{V}(s') \right\}\\
        0
    \end{bmatrix} = \begin{bmatrix}
        12.82\\
        5.89\\
        0
    \end{bmatrix}
\end{align}

We can see the optimal policy (the policy which achieves these values) is given by:
\begin{align}
    \pi_1(s) = \begin{bmatrix}
        a_2 \\
        a_2
    \end{bmatrix};\quad
    \pi_2(s) = \begin{bmatrix}
        a_1 \\
        a_2
    \end{bmatrix}
\end{align}

We can also see from the relation between $V_2(s)\ \&\ V_1(s)$ that the optiamal policy will remain at $\pi_2(s)$ for all future values. 
Since we know that $V_t(s)$ is an increasing function, i.e. we get better values each time, we can see that the relation implied by the transition probabilities in both states implies that this increasing nature will force the optimal policy to remain the same. 

This can be seen with:
\begin{align}
    s_1&: \quad 0.1(11.2) + 0.4(4.3)>2 \\
    s_2&: \quad 0.2(11.2) > 2
\end{align}
These relations guarentee that the current optimal policy (i.e. $\pi_2(s)$) will remain optimal in all future steps.



\end{document}